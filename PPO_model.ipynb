{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QA bot baseline\n",
        "## Pretrained Model Load(SKT-KoGPT2)"
      ],
      "metadata": {
        "id": "Z9Ciu62NpJax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4pxhYJlN6Kf",
        "outputId": "a857ac04-744b-4865-be06-4641a3843994"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "pad_token='<pad>', mask_token='<mask>')\n",
        "tokenizer.tokenize(\"</s> 안녕하세요. 한국어 GPT-2 입니다.😤:)l^o </s>\")"
      ],
      "metadata": {
        "id": "f1qwKQ9vrwLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eff963-a57f-4e6c-dda3-bf2408674b9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>',\n",
              " '▁안녕',\n",
              " '하',\n",
              " '세',\n",
              " '요.',\n",
              " '▁한국어',\n",
              " '▁G',\n",
              " 'P',\n",
              " 'T',\n",
              " '-2',\n",
              " '▁입',\n",
              " '니다.',\n",
              " '😤',\n",
              " ':)',\n",
              " 'l^o',\n",
              " '▁',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# JSON 파일 로드 함수\n",
        "def load_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answer = entry[\"answer\"][\"contents\"]\n",
        "        pairs.append((question, answer))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Dataset 클래스\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs, tokenizer, max_length=256):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_pairs[idx]\n",
        "        input_text = f\"</s> 질문: {question} 답변: {answer} </s>\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # labels를 input_ids의 복사본으로 생성\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # 패딩 토큰에 대한 손실을 무시하기 위해 패딩 토큰 인덱스를 -100으로 설정\n",
        "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "# 파일 경로\n",
        "train_filepath = '/content/drive/MyDrive/멋사자 AI 2차 실전 프로젝트/한국어 성능이 개선된 초거대AI 언어모델 개발 및 데이터/Training/02.라벨링데이터/TL_02.RLHF데이터/SFTlabel.json'\n",
        "valid_filepath = '/content/drive/MyDrive/멋사자 AI 2차 실전 프로젝트/한국어 성능이 개선된 초거대AI 언어모델 개발 및 데이터/Validation/02.라벨링데이터/VL/SFTlabel.json'\n",
        "\n",
        "# JSON 파일 데이터 로드\n",
        "train_pairs = load_data(train_filepath)\n",
        "valid_pairs = load_data(valid_filepath)\n",
        "\n",
        "# Tokenizer 초기화 (예: Hugging Face KoGPT2 사용)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        "    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "    pad_token='<pad>', mask_token='<mask>'\n",
        ")\n",
        "\n",
        "# Dataset 및 DataLoader 생성\n",
        "train_dataset = QADataset(train_pairs, tokenizer)\n",
        "valid_dataset = QADataset(valid_pairs, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 학습용 모델 불러오기 (예: Hugging Face GPT)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2').to(device)\n",
        "\n",
        "# 옵티마이저 및 스케줄러 정의\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 2\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# 학습 및 검증 루프\n",
        "for epoch in range(num_epochs):\n",
        "    # === 학습 단계 ===\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # === 검증 단계 ===\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Validation Loss: {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMohjsyl0O3-",
        "outputId": "7b9743f9-7686-4015-d91f-306bcb79d47b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 2.7731\n",
            "Epoch 1 | Validation Loss: 2.5639\n",
            "Epoch 2 | Train Loss: 2.4258\n",
            "Epoch 2 | Validation Loss: 2.4964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_rm_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    qa_pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answers = []\n",
        "\n",
        "        for i in range(1, 6):  # 최대 5개의 답변을 처리\n",
        "            answer_key = f\"answer{i:02d}\"\n",
        "            if answer_key in entry:\n",
        "                answer = entry[answer_key][\"contents\"]\n",
        "                answers.append(answer)\n",
        "\n",
        "        qa_pairs.append((question, answers))\n",
        "\n",
        "    return qa_pairs\n"
      ],
      "metadata": {
        "id": "75lsx6FOWSQT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# 학습된 모델 불러오기\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\").to(device)\n"
      ],
      "metadata": {
        "id": "DosXs6cMY02j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_reward_model_output(model, tokenizer, question, answers, device):\n",
        "    inputs = [f\"</s> 질문: {question} 답변: {ans} </s>\" for ans in answers]\n",
        "\n",
        "    # 각 답변에 대해 모델에 입력하여 로짓을 얻음\n",
        "    inputs_encodings = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    logits = model(**inputs_encodings).logits\n",
        "\n",
        "    # 각 답변에 대한 확률을 계산 (softmax 사용)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # 가장 높은 확률을 가진 토큰 인덱스를 기준으로 보상 점수 산출\n",
        "    reward_scores = torch.max(probs, dim=-1).values\n",
        "    return reward_scores\n"
      ],
      "metadata": {
        "id": "DdaXJRcyY9vE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# 데이터 로드\n",
        "train_filepath = '/content/drive/MyDrive/멋사자 AI 2차 실전 프로젝트/한국어 성능이 개선된 초거대AI 언어모델 개발 및 데이터/Training/02.라벨링데이터/TL_02.RLHF데이터/RMlabel.json'\n",
        "train_pairs = load_rm_data(train_filepath)\n",
        "\n",
        "# Tokenizer 초기화\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "    pad_token='<pad>', mask_token='<mask>'\n",
        ")\n",
        "\n",
        "# 옵티마이저 정의\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# DataLoader 준비\n",
        "train_dataloader = DataLoader(train_pairs, batch_size=8, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        questions, answers = batch\n",
        "\n",
        "        # 답변에 대한 보상 점수 계산\n",
        "        reward_scores = compute_reward_model_output(model, tokenizer, questions, answers, device)\n",
        "\n",
        "        # 손실 함수 정의 (예: 보상 점수에 대한 MSE 또는 CrossEntropyLoss)\n",
        "        loss = -reward_scores.mean()  # 보상 점수를 최대화하도록 학습\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "Kj72jVXYZCud",
        "outputId": "c39954c6-2750-4f3e-8ddf-ef44d30b442b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-db7c0d5a435d>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 답변에 대한 보상 점수 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mreward_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_reward_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# 손실 함수 정의 (예: 보상 점수에 대한 MSE 또는 CrossEntropyLoss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-63e95e904927>\u001b[0m in \u001b[0;36mcompute_reward_model_output\u001b[0;34m(model, tokenizer, question, answers, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 각 답변에 대해 모델에 입력하여 로짓을 얻음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 각 답변에 대한 확률을 계산 (softmax 사용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1272\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0m_use_sdpa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[0m\u001b[1;32m   1041\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mis_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torchdynamo_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     ignore_causal_mask = AttentionMaskConverter._ignore_causal_mask_sdpa(\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_ignore_causal_mask_sdpa\u001b[0;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mquery_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey_value_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0;31m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the pre-trained koGPT2 model\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Reward Model (RM) (Assumed as a placeholder for the actual Reward Model)\n",
        "class RewardModel(nn.Module):\n",
        "    def forward(self, text):\n",
        "        # Placeholder for actual reward model implementation\n",
        "        return torch.tensor([np.random.random() for _ in text])  # Random rewards for now\n",
        "\n",
        "reward_model = RewardModel()\n",
        "\n",
        "# PPO Hyperparameters\n",
        "gamma = 0.99       # Discount factor\n",
        "eps_clip = 0.2     # PPO clipping epsilon\n",
        "lr = 1e-5          # Learning rate\n",
        "betas = (0.9, 0.999)\n",
        "K_epochs = 3       # Number of PPO epochs\n",
        "batch_size = 4     # Batch size\n",
        "max_timesteps = 1000  # Max timesteps per episode\n",
        "eps = 1e-6\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "# Define PPO policy model\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, model, tokenizer, reward_model):\n",
        "        super(PPO, self).__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.reward_model = reward_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n",
        "    def get_action(self, input_ids, attention_mask):\n",
        "        logits = self.model(input_ids, attention_mask=attention_mask).logits\n",
        "        dist = Categorical(F.softmax(logits, dim=-1))\n",
        "        action = dist.sample()\n",
        "        return action, dist\n",
        "\n",
        "ppo_model = PPO(model, tokenizer, reward_model)\n",
        "\n",
        "# Sample environment (Dummy Environment for Text Generation Task)\n",
        "class TextGenEnv(gym.Env):\n",
        "    def __init__(self, tokenizer):\n",
        "        super(TextGenEnv, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.current_state = None\n",
        "        self.max_len = 50  # Max sequence length\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = self.tokenizer.encode(\"안녕하세요, 오늘 날씨는\", return_tensors=\"pt\")  # Start sentence\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Add the token from the action into the sequence\n",
        "        new_state = torch.cat([self.current_state, action.unsqueeze(0)], dim=-1)\n",
        "\n",
        "        # Reward Calculation based on Reward Model\n",
        "        reward = self.reward_model(new_state)\n",
        "\n",
        "        # Check if the length exceeded the max length (terminate if so)\n",
        "        done = len(new_state[0]) >= self.max_len\n",
        "        return new_state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        return self.tokenizer.decode(self.current_state[0], skip_special_tokens=True)\n",
        "\n",
        "# Instantiate environment and PPO agent\n",
        "env = TextGenEnv(tokenizer)\n",
        "ppo_model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(K_epochs):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        input_ids = state\n",
        "        attention_mask = torch.ones_like(input_ids)  # Assuming no padding for simplicity\n",
        "\n",
        "        # Get the action (next token prediction) and the probability distribution\n",
        "        action, dist = ppo_model.get_action(input_ids, attention_mask)\n",
        "\n",
        "        # Step through the environment with the action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Calculate the advantage\n",
        "        reward = reward.item()  # Convert tensor to scalar\n",
        "\n",
        "        # Calculate the log probability of the action taken\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Compute the PPO objective (loss)\n",
        "        loss = -log_prob * reward  # The reward would be the advantage\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Reward: {total_reward}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cBobP7RoWDX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Model Test"
      ],
      "metadata": {
        "id": "tKe1BPlf4L5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '</s> 근육을 키우려면'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntqoXSr-XtUw",
        "outputId": "a808ef71-669c-46fe-be59-e5817f923c58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 근육을 키우려면 운동량을 늘려야 한다.\n",
            "또한 운동을 할 때 체중을 줄이거나 식이요법을 병행하는 것이 좋다.\n",
            "운동량이 부족하면 근육이 약해져 살이 찌기 쉽다.\n",
            "이때는 적당한 운동과 함께 충분한 수면을 취하는 것도 중요하다.\n",
            "운동을 하면 근육의 양이 늘어나게 되고 이는 곧 지방 축적을 촉진하게 된다.\n",
            "따라서 운동은 체중 감량에 도움이 되지만 무리해서 하는 것은 오히려 독이 될 수도 있다.\n",
            "운동은 하루 30분 정도만 해도 효과가 있지만 일주일에 한 번 정도는 반드시 해야 한다.</d> 지난해 12월 31일 오후 2시쯤 서울 강남구 역삼동 강남역 인근에서 A(30)씨가 몰던 승용차가 중앙선을 넘어 마주\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '</s> 청소를 할 때 가장 먼저 해야할 것은'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "mTs936-j0O10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2233898-b379-43ca-ed61-a1dd7c8fac98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 청소를 할 때 가장 먼저 해야할 것은 바로 청소다.\n",
            "청소 후에는 반드시 세정제를 이용해 깨끗이 씻어내고 먼지를 털어내야 한다.\n",
            "세탁 후 물기가 마르기 전에 세탁물을 충분히 헹궈낸 다음 물기를 제거한 뒤 중성 세제 등으로 닦아내면 된다.\n",
            "또한 빨래 건조기를 사용해 젖은 옷이나 이불 등을 말끔히 말려주는 것도 잊지 말아야 한다.</d> 지난해 12월 31일 오후 2시 서울 강남구 삼성동 코엑스 3층 컨퍼런스코에서 열린 ‘2018 대한민국 국제의료기기전시회(KIMES 2018)’에서 만난 김지현 원장은 “국내외 의료기기를 한 자리에서 볼 수 있는\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "auoElfwb4RfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answer = entry[\"answer\"][\"contents\"]\n",
        "        pairs.append((question, answer))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "filepath = 'SFTlabel.json'\n",
        "pairs = load_data(filepath)\n",
        "print(pairs[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojpLLfh50Ozc",
        "outputId": "ee27b68d-4318-4868-c881-996a5b612e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('유리공예 작업할 때 필요한 도구와 재료가 뭐야?', '유리공예 작업을 위해 필요한 도구 및 재료는 다음과 같습니다:\\n\\n도구\\n1. 유리 절단 칼: 유리를 원하는 크기로 절단하기 위해 사용됩니다.\\n2. 선과: 직선을 그리기 위해 사용됩니다.\\n3. 절단 접착제: 접합을 위해 사용되며, 강력한 접착제로 유리 조각이 붙는 데 도움을 줍니다.\\n4. 모래 지속기: 유리의 끝을 다듬기 위해 사용됩니다.\\n5. 소화기: 작업 도중 작은 불꽃이 발생할 경우 안전을 위해 사용됩니다.\\n6. 연마 도구: 유리의 표면을 연마하여 다듬을 수 있습니다.\\n7. 벤치 그라인더: 유리를 모양을 다듬는 데 사용되며, 모양을 좀 더 정확하게 조정할 수 있습니다.\\n8. 페인트 브러시: 디자인과 색상을 유리에 그릴 때 사용됩니다.\\n\\n재료\\n1. 유리 조각: 다양한 크기, 모양, 색상의 유리 조각을 사용할 수 있습니다.\\n2. 테두리: 유리 작품을 보호하거나 견고하게 만들기 위해 사용됩니다.\\n3. 줄무늬: 작품을 걸거나 장식할 때 사용되며, 다양한 색상과 재질을 사용할 수 있습니다.\\n4. 트림: 테두리를 꾸미거나 작품의 외관을 강조하기 위해 사용됩니다.\\n5. 유리그릇: 작품에 사용될 수 있는 꽃, 모래, 컬러드 샌드 등의 재료를 담을 수 있습니다.\\n6. 폴리싱 제품: 유리의 표면을 광택 내기 위해 사용됩니다.\\n7. 에나멜 그릇: 유리에 그림을 그리고 색칠하기 위해 사용됩니다.\\n8. 용제: 유리 조각을 부착하거나 다른 재료를 붙이기 위해 사용됩니다.\\n\\n다양한 도구와 재료를 사용하여 유리공예 작업을 즐기고 다양한 작품을 만들 수 있습니다!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in model.transformer.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# for param in model.lm_head.parameters():\n",
        "#     param.requires_grad = True"
      ],
      "metadata": {
        "id": "8d53ahWVG08F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs, tokenizer, max_length=256):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_pairs[idx]\n",
        "        input_text = f\"</s> 질문: {question} 답변: {answer} </s>\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # labels를 input_ids의 복사본으로 생성\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # 패딩 토큰에 대한 손실을 무시하기 위해 패딩 토큰 인덱스를 -100으로 설정\n",
        "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "WsdH5ZCV0Osd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Dataset과 DataLoader 생성\n",
        "dataset = QADataset(pairs, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# DataLoader에서 배치 샘플 확인\n",
        "for batch in dataloader:\n",
        "    print(batch['input_ids'].shape)  # (batch_size, max_length)\n",
        "    print(batch['attention_mask'].shape)  # (batch_size, max_length)\n",
        "    print(batch['labels'].shape)  # (batch_size, max_length)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFEOzt5h33G8",
        "outputId": "c5358b62-f4a0-46ba-ca3b-c17e46f5fd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 256])\n",
            "torch.Size([16, 256])\n",
            "torch.Size([16, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Fine-tuning"
      ],
      "metadata": {
        "id": "j5RsVxtY4aXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "# Dataset과 DataLoader 정의\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# 옵티마이저와 스케줄러 정의\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "total_steps = len(dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snMOEgHA33D8",
        "outputId": "9497ab20-3d4a-4eac-d326-d516a8ad33cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.743095874786377\n",
            "Epoch: 1, Loss: 2.508897542953491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "# JSON 파일 로드 함수\n",
        "def load_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answer = entry[\"answer\"][\"contents\"]\n",
        "        pairs.append((question, answer))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Dataset 클래스\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs, tokenizer, max_length=256):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_pairs[idx]\n",
        "        input_text = f\"</s> 질문: {question} 답변: {answer} </s>\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # labels를 input_ids의 복사본으로 생성\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # 패딩 토큰에 대한 손실을 무시하기 위해 패딩 토큰 인덱스를 -100으로 설정\n",
        "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "# 파일 경로\n",
        "train_filepath = '/content/drive/MyDrive/멋사자 AI 2차 실전 프로젝트/한국어 성능이 개선된 초거대AI 언어모델 개발 및 데이터/Training/02.라벨링데이터/TL_02.RLHF데이터/SFTlabel.json'\n",
        "valid_filepath = '/content/drive/MyDrive/멋사자 AI 2차 실전 프로젝트/한국어 성능이 개선된 초거대AI 언어모델 개발 및 데이터/Validation/02.라벨링데이터/VL/SFTlabel.json'\n",
        "\n",
        "# JSON 파일 데이터 로드\n",
        "train_pairs = load_data(train_filepath)\n",
        "valid_pairs = load_data(valid_filepath)\n",
        "\n",
        "# Tokenizer 초기화 (예: Hugging Face KoGPT2 사용)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
        "\n",
        "# Dataset 및 DataLoader 생성\n",
        "train_dataset = QADataset(train_pairs, tokenizer)\n",
        "valid_dataset = QADataset(valid_pairs, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 학습용 모델 불러오기 (예: Hugging Face GPT)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\").to(device)\n",
        "\n",
        "# 옵티마이저 및 스케줄러 정의\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 2\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# 학습 및 검증 루프\n",
        "for epoch in range(num_epochs):\n",
        "    # === 학습 단계 ===\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # === 검증 단계 ===\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Validation Loss: {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "QZk1RxCePs5n",
        "outputId": "9aa50b6b-568c-4415-f508-3f7f0ee00574"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-099624a4201d>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-099624a4201d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"</s> 질문: {question} 답변: {answer} </s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         encoding = self.tokenizer(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m             )\n\u001b[1;32m   3130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3131\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   3132\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3198\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3199\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3200\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2921\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2923\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2924\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA bot Test"
      ],
      "metadata": {
        "id": "gBvH-BF24c5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '근육을 키우려면 어떻게 해야할까요?'\n",
        "input_text = f\"</s> 질문: {text} 답변:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVOIKxnLlFPw",
        "outputId": "858c2ce1-bbd8-48f5-b900-c81e3b3331ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 질문: 근육을 키우려면 어떻게 해야할까요? 답변: 근육이 형성되는 과정은 다음과 같습니다 :\n",
            "1. 균형 잡힌 운동 및 식단 관리 - 근력을 키우기 위해 적절한 운동을 시작합니다. \n",
            "2. 스트레스를 관리하기 위한 식사 조절하기, 스트레칭과 같은 유산소 운동과 함께 하는 활동 등을 포함합니다.\n",
            "3. 신체활동 계획 수립- 건강한 신체를 유지하기 위해서는 충분한 휴식과 수면을 취하는 것이 중요합니다,\n",
            " 규칙적인 생활과 꾸준한 운동은 근육의 활동을 촉진하고 긴장을 완화시키는 데 도움이 됩니다. 또한, 과도한 음식은 피해야 합니다. 캥거루나 바렛더 등의 가벼운 스포츠 활동은 몸의 균형을 깨뜨려 부상을 예방하거나\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '청소를 할 때 무엇을 먼저 해야할까요?'\n",
        "input_text = f\"</s> 질문: {text} 답변:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "ZyQXhYSi33BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cd3bbe-64cc-4259-b597-a2872da7e99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 질문: 청소를 할 때 무엇을 먼저 해야할까요? 답변: 청소 후에는 다음과 같은 단계를 거쳐야 합니다 :\n",
            "1. 먼지 제거 및 클리닝을 위한 세트 사용법 설명하시오. \n",
            "2. 세탁기 필터링과 공기청정기 사용을 위해 세척기를 사용해야 합니다. 이 경우 세제 또는 섬유유를 사용하여 깨끗하게 닦아낼 수 있습니다. 다음은 몇 가지 예시입니다\n",
            "- 팁(Tip) - 깨끗한 물을 사용하는 것이 중요합니다. 이는 오염물질이나 세균을 효과적으로 제거하는 데 도움이 됩니다. 또한, 오염된 물도 사용할 필요가 있습니다.\n",
            "3. 적절한 물과 함께 사용하면 더 잘 닦일 수도 있습니다(Therm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"산림의 주요 기능은 무엇이며, 환경 보호와 관련하여 어떤 역할을 하나요?\"\n",
        "input_text = f\"</s> 질문: {text} 답변:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "62XCExZ8cVga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8162494a-50cf-459a-b873-73fe303ecf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 질문: 산림의 주요 기능은 무엇이며, 환경 보호와 관련하여 어떤 역할을 하나요? 답변: 산림 보호는 다양한 역할과 책임을 가지고 있습니다. \n",
            "\n",
            "1. 생태계 보전 및 지속 가능한 이용 촉진 : 산림은 생물 다양성과 생태적 다양성을 유지하고 유지하는데 중요한 기능을 합니다. 이는 다음과 같은 역할들을 포함합니다.\n",
            "첫째, 생태계의 보전과 복원\"은 기후변화와 자연재해로부터 보호하는 데 도움을 줍니다. 이를 위해 정부는 숲 가꾸기, 나무 심기 등 숲을 위한 사업을 추진합니다. 이러한 사업은 숲의 가치를 높이고 환경을 보호하기 때문에 많은 사람들이 혜택을 볼 수 있습니다.\n",
            "둘째로, 임업 생산성 향상 (예: 목재 생산) 은 임산물 생산을 통해 수익을 창출할 수도 있지만, 다른\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Save"
      ],
      "metadata": {
        "id": "YhYdBqGS5Jf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = './fine_tuned_kogpt2'\n",
        "\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "LRiF0R2ng9fG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182902f6-ffeb-40ba-b5e7-e102671d8796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_kogpt2/tokenizer_config.json',\n",
              " './fine_tuned_kogpt2/special_tokens_map.json',\n",
              " './fine_tuned_kogpt2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Load"
      ],
      "metadata": {
        "id": "D5Jq8XVp5NVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(save_directory)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNLT5APq5Qa6",
        "outputId": "b16ac2d8-eab0-49a2-d615-6bc7a5443c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '근육을 키우려면 어떻게 해야할까요?'\n",
        "input_text = f\"</s> 질문: {text} 답변:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjnHmxS5UnW",
        "outputId": "07ceb51f-6e7f-427d-d372-18d9a39212fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 질문: 근육을 키우려면 어떻게 해야할까요? 답변: 근육이 성장하려면 다음과 같은 단계를 거쳐야 합니다 :\n",
            "1. 운동량 조절 - 근력 운동을 통해 근육의 힘을 키웁니다. \n",
            "2. 스트레칭과 유산소운동- 그리고 균형 잡힌 식단 유지와 적절한 운동으로 체중을 조절하는 것이 중요합니다.\n",
            "3. 유연성 향상 및 밸런스 관리– 유연한 신체 구조를 유지하는 것은 중요합니다. 예를 들어, 발바닥, 팔뚝 등 다양한 부위의 움직임을 부드럽게 만들어주는 동작을 반복하면 됩니다.\n",
            "4. 호흡 훈련의 중요성 확인 ­ 호흡을 통한 몸의 움직임에 대한 이해와 훈련을 실시해야 합니다. 이를 위해 충분한 휴식과 함께 규칙적인 수면\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"산림의 주요 기능은 무엇이며, 환경 보호와 관련하여 어떤 역할을 하나요?\"\n",
        "input_text = f\"</s> 질문: {text} 답변:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSNbIF_C5Xpv",
        "outputId": "e421d8a3-8a39-47df-a914-e87bcaf02354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> 질문: 산림의 주요 기능은 무엇이며, 환경 보호와 관련하여 어떤 역할을 하나요? 답변: 산림청은 다양한 기능을 수행하는 대표적인 임업 기관입니다. \n",
            "\n",
            "1. 생태계 보전 및 관리 : 산림은 생물 다양성과 생태계를 보호하고, 생태계의 균형을 유지합니다. 이를 위해 숲 가꾸기, 나무 심기 등 자연친화적인 활동을 장려하고 지속 가능한 관리를 제공합니다.\n",
            "2. 기후변화 대응과 자원 재활용 촉진 노력 - 정부는 온실가스 배출을 줄이기 위한 노력을 기울이고 있습니다.\n",
            "3. 수질 개선 활동 지원 (예: 대기오염 정화) 사업은 하천의 수질을 개선하기 위하여 물을 공급하고 오염물질을 제거하는 데 도움을 줍니다.\n",
            "4. 재해 예방활동 강화와 복구 계획 수립에 대한 지원을 통해 국민의\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### enhancing variability"
      ],
      "metadata": {
        "id": "bDQmVX8a8FB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"산림의 주요 기능은 무엇이며, 환경 보호와 관련하여 어떤 역할을 하나요?\"\n",
        "input_text = f\"</s> 질문: {text} 답변:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "gen_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=128,\n",
        "    repetition_penalty=2.0,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    use_cache=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXp1kGvI7dzP",
        "outputId": "f47f2973-54cd-4fa5-e160-b6640109357b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: 산림의 주요 기능은 무엇이며, 환경 보호와 관련하여 어떤 역할을 하나요? 답변: 산림 생태계의 중요한 역할은 다음과 같습니다 :\n",
            "1. 자원 보존 및 보전에 관한 기본 원칙, 보전 계획 수립과 유지 보수 절차의 준수 여부 등입니다.\n",
            "2. 생태계 복원과 보호 관리 계획의 수립, 지속 가능한 관리를 위한 노력 등을 포함합니다.\n",
            "3. 기후 변화 대응에 대한 관심과 참여도 필요합니다. 이를 위해 다양한 정책 제안이 이루어집니다.\n",
            "4. 생물 다양성 보호를 통한 서식지 보호, 토양 오염 예방 등에 관심을 기울여야 합니다.\n",
            "5. 자연 재해 예방을 위해서는 지속적인 관리와 관리가 필요합니다.\n",
            "6. 대기 질 개선을 통해 숲 훼손을 방지하고 숲의 생태적 가치를 증진\n"
          ]
        }
      ]
    }
  ]
}