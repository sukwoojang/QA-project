{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QA bot baseline\n",
        "## Pretrained Model Load(SKT-KoGPT2)"
      ],
      "metadata": {
        "id": "Z9Ciu62NpJax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4pxhYJlN6Kf",
        "outputId": "a857ac04-744b-4865-be06-4641a3843994"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "pad_token='<pad>', mask_token='<mask>')\n",
        "tokenizer.tokenize(\"</s> ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o </s>\")"
      ],
      "metadata": {
        "id": "f1qwKQ9vrwLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eff963-a57f-4e6c-dda3-bf2408674b9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>',\n",
              " 'â–ì•ˆë…•',\n",
              " 'í•˜',\n",
              " 'ì„¸',\n",
              " 'ìš”.',\n",
              " 'â–í•œêµ­ì–´',\n",
              " 'â–G',\n",
              " 'P',\n",
              " 'T',\n",
              " '-2',\n",
              " 'â–ì…',\n",
              " 'ë‹ˆë‹¤.',\n",
              " 'ğŸ˜¤',\n",
              " ':)',\n",
              " 'l^o',\n",
              " 'â–',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# JSON íŒŒì¼ ë¡œë“œ í•¨ìˆ˜\n",
        "def load_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answer = entry[\"answer\"][\"contents\"]\n",
        "        pairs.append((question, answer))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Dataset í´ë˜ìŠ¤\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs, tokenizer, max_length=256):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_pairs[idx]\n",
        "        input_text = f\"</s> ì§ˆë¬¸: {question} ë‹µë³€: {answer} </s>\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # labelsë¥¼ input_idsì˜ ë³µì‚¬ë³¸ìœ¼ë¡œ ìƒì„±\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # íŒ¨ë”© í† í°ì— ëŒ€í•œ ì†ì‹¤ì„ ë¬´ì‹œí•˜ê¸° ìœ„í•´ íŒ¨ë”© í† í° ì¸ë±ìŠ¤ë¥¼ -100ìœ¼ë¡œ ì„¤ì •\n",
        "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "train_filepath = '/content/drive/MyDrive/ë©‹ì‚¬ì AI 2ì°¨ ì‹¤ì „ í”„ë¡œì íŠ¸/í•œêµ­ì–´ ì„±ëŠ¥ì´ ê°œì„ ëœ ì´ˆê±°ëŒ€AI ì–¸ì–´ëª¨ë¸ ê°œë°œ ë° ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°/TL_02.RLHFë°ì´í„°/SFTlabel.json'\n",
        "valid_filepath = '/content/drive/MyDrive/ë©‹ì‚¬ì AI 2ì°¨ ì‹¤ì „ í”„ë¡œì íŠ¸/í•œêµ­ì–´ ì„±ëŠ¥ì´ ê°œì„ ëœ ì´ˆê±°ëŒ€AI ì–¸ì–´ëª¨ë¸ ê°œë°œ ë° ë°ì´í„°/Validation/02.ë¼ë²¨ë§ë°ì´í„°/VL/SFTlabel.json'\n",
        "\n",
        "# JSON íŒŒì¼ ë°ì´í„° ë¡œë“œ\n",
        "train_pairs = load_data(train_filepath)\n",
        "valid_pairs = load_data(valid_filepath)\n",
        "\n",
        "# Tokenizer ì´ˆê¸°í™” (ì˜ˆ: Hugging Face KoGPT2 ì‚¬ìš©)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        "    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "    pad_token='<pad>', mask_token='<mask>'\n",
        ")\n",
        "\n",
        "# Dataset ë° DataLoader ìƒì„±\n",
        "train_dataset = QADataset(train_pairs, tokenizer)\n",
        "valid_dataset = QADataset(valid_pairs, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# í•™ìŠµìš© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: Hugging Face GPT)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2').to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 2\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# í•™ìŠµ ë° ê²€ì¦ ë£¨í”„\n",
        "for epoch in range(num_epochs):\n",
        "    # === í•™ìŠµ ë‹¨ê³„ ===\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # === ê²€ì¦ ë‹¨ê³„ ===\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Validation Loss: {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMohjsyl0O3-",
        "outputId": "7b9743f9-7686-4015-d91f-306bcb79d47b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 2.7731\n",
            "Epoch 1 | Validation Loss: 2.5639\n",
            "Epoch 2 | Train Loss: 2.4258\n",
            "Epoch 2 | Validation Loss: 2.4964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_rm_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    qa_pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answers = []\n",
        "\n",
        "        for i in range(1, 6):  # ìµœëŒ€ 5ê°œì˜ ë‹µë³€ì„ ì²˜ë¦¬\n",
        "            answer_key = f\"answer{i:02d}\"\n",
        "            if answer_key in entry:\n",
        "                answer = entry[answer_key][\"contents\"]\n",
        "                answers.append(answer)\n",
        "\n",
        "        qa_pairs.append((question, answers))\n",
        "\n",
        "    return qa_pairs\n"
      ],
      "metadata": {
        "id": "75lsx6FOWSQT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\").to(device)\n"
      ],
      "metadata": {
        "id": "DosXs6cMY02j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_reward_model_output(model, tokenizer, question, answers, device):\n",
        "    inputs = [f\"</s> ì§ˆë¬¸: {question} ë‹µë³€: {ans} </s>\" for ans in answers]\n",
        "\n",
        "    # ê° ë‹µë³€ì— ëŒ€í•´ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë¡œì§“ì„ ì–»ìŒ\n",
        "    inputs_encodings = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    logits = model(**inputs_encodings).logits\n",
        "\n",
        "    # ê° ë‹µë³€ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚° (softmax ì‚¬ìš©)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í° ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³´ìƒ ì ìˆ˜ ì‚°ì¶œ\n",
        "    reward_scores = torch.max(probs, dim=-1).values\n",
        "    return reward_scores\n"
      ],
      "metadata": {
        "id": "DdaXJRcyY9vE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "train_filepath = '/content/drive/MyDrive/á„†á…¥á†ºá„‰á…¡á„Œá…¡ AI 2á„á…¡ á„‰á…µá†¯á„Œá…¥á†« á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„’á…¡á†«á„€á…®á†¨á„‹á…¥ á„‰á…¥á†¼á„‚á…³á†¼á„‹á…µ á„€á…¢á„‰á…¥á†«á„ƒá…¬á†« á„á…©á„€á…¥á„ƒá…¢AI á„‹á…¥á†«á„‹á…¥á„†á…©á„ƒá…¦á†¯ á„€á…¢á„‡á…¡á†¯ á„†á…µá†¾ á„ƒá…¦á„‹á…µá„á…¥/Training/02.á„…á…¡á„‡á…¦á†¯á„…á…µá†¼á„ƒá…¦á„‹á…µá„á…¥/TL_02.RLHFá„ƒá…¦á„‹á…µá„á…¥/RMlabel.json'\n",
        "train_pairs = load_rm_data(train_filepath)\n",
        "\n",
        "# Tokenizer ì´ˆê¸°í™”\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "    pad_token='<pad>', mask_token='<mask>'\n",
        ")\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# DataLoader ì¤€ë¹„\n",
        "train_dataloader = DataLoader(train_pairs, batch_size=8, shuffle=True)\n",
        "\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        questions, answers = batch\n",
        "\n",
        "        # ë‹µë³€ì— ëŒ€í•œ ë³´ìƒ ì ìˆ˜ ê³„ì‚°\n",
        "        reward_scores = compute_reward_model_output(model, tokenizer, questions, answers, device)\n",
        "\n",
        "        # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (ì˜ˆ: ë³´ìƒ ì ìˆ˜ì— ëŒ€í•œ MSE ë˜ëŠ” CrossEntropyLoss)\n",
        "        loss = -reward_scores.mean()  # ë³´ìƒ ì ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ í•™ìŠµ\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "Kj72jVXYZCud",
        "outputId": "c39954c6-2750-4f3e-8ddf-ef44d30b442b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-db7c0d5a435d>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# ë‹µë³€ì— ëŒ€í•œ ë³´ìƒ ì ìˆ˜ ê³„ì‚°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mreward_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_reward_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (ì˜ˆ: ë³´ìƒ ì ìˆ˜ì— ëŒ€í•œ MSE ë˜ëŠ” CrossEntropyLoss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-63e95e904927>\u001b[0m in \u001b[0;36mcompute_reward_model_output\u001b[0;34m(model, tokenizer, question, answers, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# ê° ë‹µë³€ì— ëŒ€í•´ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë¡œì§“ì„ ì–»ìŒ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# ê° ë‹µë³€ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚° (softmax ì‚¬ìš©)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1272\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0m_use_sdpa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[0m\u001b[1;32m   1041\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mis_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torchdynamo_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     ignore_causal_mask = AttentionMaskConverter._ignore_causal_mask_sdpa(\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_ignore_causal_mask_sdpa\u001b[0;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mquery_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey_value_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0;31m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the pre-trained koGPT2 model\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Reward Model (RM) (Assumed as a placeholder for the actual Reward Model)\n",
        "class RewardModel(nn.Module):\n",
        "    def forward(self, text):\n",
        "        # Placeholder for actual reward model implementation\n",
        "        return torch.tensor([np.random.random() for _ in text])  # Random rewards for now\n",
        "\n",
        "reward_model = RewardModel()\n",
        "\n",
        "# PPO Hyperparameters\n",
        "gamma = 0.99       # Discount factor\n",
        "eps_clip = 0.2     # PPO clipping epsilon\n",
        "lr = 1e-5          # Learning rate\n",
        "betas = (0.9, 0.999)\n",
        "K_epochs = 3       # Number of PPO epochs\n",
        "batch_size = 4     # Batch size\n",
        "max_timesteps = 1000  # Max timesteps per episode\n",
        "eps = 1e-6\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "# Define PPO policy model\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, model, tokenizer, reward_model):\n",
        "        super(PPO, self).__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.reward_model = reward_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n",
        "    def get_action(self, input_ids, attention_mask):\n",
        "        logits = self.model(input_ids, attention_mask=attention_mask).logits\n",
        "        dist = Categorical(F.softmax(logits, dim=-1))\n",
        "        action = dist.sample()\n",
        "        return action, dist\n",
        "\n",
        "ppo_model = PPO(model, tokenizer, reward_model)\n",
        "\n",
        "# Sample environment (Dummy Environment for Text Generation Task)\n",
        "class TextGenEnv(gym.Env):\n",
        "    def __init__(self, tokenizer):\n",
        "        super(TextGenEnv, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.current_state = None\n",
        "        self.max_len = 50  # Max sequence length\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = self.tokenizer.encode(\"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”\", return_tensors=\"pt\")  # Start sentence\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Add the token from the action into the sequence\n",
        "        new_state = torch.cat([self.current_state, action.unsqueeze(0)], dim=-1)\n",
        "\n",
        "        # Reward Calculation based on Reward Model\n",
        "        reward = self.reward_model(new_state)\n",
        "\n",
        "        # Check if the length exceeded the max length (terminate if so)\n",
        "        done = len(new_state[0]) >= self.max_len\n",
        "        return new_state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        return self.tokenizer.decode(self.current_state[0], skip_special_tokens=True)\n",
        "\n",
        "# Instantiate environment and PPO agent\n",
        "env = TextGenEnv(tokenizer)\n",
        "ppo_model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(K_epochs):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        input_ids = state\n",
        "        attention_mask = torch.ones_like(input_ids)  # Assuming no padding for simplicity\n",
        "\n",
        "        # Get the action (next token prediction) and the probability distribution\n",
        "        action, dist = ppo_model.get_action(input_ids, attention_mask)\n",
        "\n",
        "        # Step through the environment with the action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Calculate the advantage\n",
        "        reward = reward.item()  # Convert tensor to scalar\n",
        "\n",
        "        # Calculate the log probability of the action taken\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Compute the PPO objective (loss)\n",
        "        loss = -log_prob * reward  # The reward would be the advantage\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Reward: {total_reward}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cBobP7RoWDX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Model Test"
      ],
      "metadata": {
        "id": "tKe1BPlf4L5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '</s> ê·¼ìœ¡ì„ í‚¤ìš°ë ¤ë©´'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntqoXSr-XtUw",
        "outputId": "a808ef71-669c-46fe-be59-e5817f923c58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ê·¼ìœ¡ì„ í‚¤ìš°ë ¤ë©´ ìš´ë™ëŸ‰ì„ ëŠ˜ë ¤ì•¼ í•œë‹¤.\n",
            "ë˜í•œ ìš´ë™ì„ í•  ë•Œ ì²´ì¤‘ì„ ì¤„ì´ê±°ë‚˜ ì‹ì´ìš”ë²•ì„ ë³‘í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n",
            "ìš´ë™ëŸ‰ì´ ë¶€ì¡±í•˜ë©´ ê·¼ìœ¡ì´ ì•½í•´ì ¸ ì‚´ì´ ì°Œê¸° ì‰½ë‹¤.\n",
            "ì´ë•ŒëŠ” ì ë‹¹í•œ ìš´ë™ê³¼ í•¨ê»˜ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ë‹¤.\n",
            "ìš´ë™ì„ í•˜ë©´ ê·¼ìœ¡ì˜ ì–‘ì´ ëŠ˜ì–´ë‚˜ê²Œ ë˜ê³  ì´ëŠ” ê³§ ì§€ë°© ì¶•ì ì„ ì´‰ì§„í•˜ê²Œ ëœë‹¤.\n",
            "ë”°ë¼ì„œ ìš´ë™ì€ ì²´ì¤‘ ê°ëŸ‰ì— ë„ì›€ì´ ë˜ì§€ë§Œ ë¬´ë¦¬í•´ì„œ í•˜ëŠ” ê²ƒì€ ì˜¤íˆë ¤ ë…ì´ ë  ìˆ˜ë„ ìˆë‹¤.\n",
            "ìš´ë™ì€ í•˜ë£¨ 30ë¶„ ì •ë„ë§Œ í•´ë„ íš¨ê³¼ê°€ ìˆì§€ë§Œ ì¼ì£¼ì¼ì— í•œ ë²ˆ ì •ë„ëŠ” ë°˜ë“œì‹œ í•´ì•¼ í•œë‹¤.</d> ì§€ë‚œí•´ 12ì›” 31ì¼ ì˜¤í›„ 2ì‹œì¯¤ ì„œìš¸ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™ ê°•ë‚¨ì—­ ì¸ê·¼ì—ì„œ A(30)ì”¨ê°€ ëª°ë˜ ìŠ¹ìš©ì°¨ê°€ ì¤‘ì•™ì„ ì„ ë„˜ì–´ ë§ˆì£¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '</s> ì²­ì†Œë¥¼ í•  ë•Œ ê°€ì¥ ë¨¼ì € í•´ì•¼í•  ê²ƒì€'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "mTs936-j0O10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2233898-b379-43ca-ed61-a1dd7c8fac98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ì²­ì†Œë¥¼ í•  ë•Œ ê°€ì¥ ë¨¼ì € í•´ì•¼í•  ê²ƒì€ ë°”ë¡œ ì²­ì†Œë‹¤.\n",
            "ì²­ì†Œ í›„ì—ëŠ” ë°˜ë“œì‹œ ì„¸ì •ì œë¥¼ ì´ìš©í•´ ê¹¨ë—ì´ ì”»ì–´ë‚´ê³  ë¨¼ì§€ë¥¼ í„¸ì–´ë‚´ì•¼ í•œë‹¤.\n",
            "ì„¸íƒ í›„ ë¬¼ê¸°ê°€ ë§ˆë¥´ê¸° ì „ì— ì„¸íƒë¬¼ì„ ì¶©ë¶„íˆ í—¹ê¶ˆë‚¸ ë‹¤ìŒ ë¬¼ê¸°ë¥¼ ì œê±°í•œ ë’¤ ì¤‘ì„± ì„¸ì œ ë“±ìœ¼ë¡œ ë‹¦ì•„ë‚´ë©´ ëœë‹¤.\n",
            "ë˜í•œ ë¹¨ë˜ ê±´ì¡°ê¸°ë¥¼ ì‚¬ìš©í•´ ì –ì€ ì˜·ì´ë‚˜ ì´ë¶ˆ ë“±ì„ ë§ë”íˆ ë§ë ¤ì£¼ëŠ” ê²ƒë„ ìŠì§€ ë§ì•„ì•¼ í•œë‹¤.</d> ì§€ë‚œí•´ 12ì›” 31ì¼ ì˜¤í›„ 2ì‹œ ì„œìš¸ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ ì½”ì—‘ìŠ¤ 3ì¸µ ì»¨í¼ëŸ°ìŠ¤ì½”ì—ì„œ ì—´ë¦° â€˜2018 ëŒ€í•œë¯¼êµ­ êµ­ì œì˜ë£Œê¸°ê¸°ì „ì‹œíšŒ(KIMES 2018)â€™ì—ì„œ ë§Œë‚œ ê¹€ì§€í˜„ ì›ì¥ì€ â€œêµ­ë‚´ì™¸ ì˜ë£Œê¸°ê¸°ë¥¼ í•œ ìë¦¬ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "auoElfwb4RfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answer = entry[\"answer\"][\"contents\"]\n",
        "        pairs.append((question, answer))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "filepath = 'SFTlabel.json'\n",
        "pairs = load_data(filepath)\n",
        "print(pairs[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojpLLfh50Ozc",
        "outputId": "ee27b68d-4318-4868-c881-996a5b612e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ìœ ë¦¬ê³µì˜ˆ ì‘ì—…í•  ë•Œ í•„ìš”í•œ ë„êµ¬ì™€ ì¬ë£Œê°€ ë­ì•¼?', 'ìœ ë¦¬ê³µì˜ˆ ì‘ì—…ì„ ìœ„í•´ í•„ìš”í•œ ë„êµ¬ ë° ì¬ë£ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\në„êµ¬\\n1. ìœ ë¦¬ ì ˆë‹¨ ì¹¼: ìœ ë¦¬ë¥¼ ì›í•˜ëŠ” í¬ê¸°ë¡œ ì ˆë‹¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n2. ì„ ê³¼: ì§ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n3. ì ˆë‹¨ ì ‘ì°©ì œ: ì ‘í•©ì„ ìœ„í•´ ì‚¬ìš©ë˜ë©°, ê°•ë ¥í•œ ì ‘ì°©ì œë¡œ ìœ ë¦¬ ì¡°ê°ì´ ë¶™ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\\n4. ëª¨ë˜ ì§€ì†ê¸°: ìœ ë¦¬ì˜ ëì„ ë‹¤ë“¬ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n5. ì†Œí™”ê¸°: ì‘ì—… ë„ì¤‘ ì‘ì€ ë¶ˆê½ƒì´ ë°œìƒí•  ê²½ìš° ì•ˆì „ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n6. ì—°ë§ˆ ë„êµ¬: ìœ ë¦¬ì˜ í‘œë©´ì„ ì—°ë§ˆí•˜ì—¬ ë‹¤ë“¬ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n7. ë²¤ì¹˜ ê·¸ë¼ì¸ë”: ìœ ë¦¬ë¥¼ ëª¨ì–‘ì„ ë‹¤ë“¬ëŠ” ë° ì‚¬ìš©ë˜ë©°, ëª¨ì–‘ì„ ì¢€ ë” ì •í™•í•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n8. í˜ì¸íŠ¸ ë¸ŒëŸ¬ì‹œ: ë””ìì¸ê³¼ ìƒ‰ìƒì„ ìœ ë¦¬ì— ê·¸ë¦´ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\nì¬ë£Œ\\n1. ìœ ë¦¬ ì¡°ê°: ë‹¤ì–‘í•œ í¬ê¸°, ëª¨ì–‘, ìƒ‰ìƒì˜ ìœ ë¦¬ ì¡°ê°ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n2. í…Œë‘ë¦¬: ìœ ë¦¬ ì‘í’ˆì„ ë³´í˜¸í•˜ê±°ë‚˜ ê²¬ê³ í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n3. ì¤„ë¬´ëŠ¬: ì‘í’ˆì„ ê±¸ê±°ë‚˜ ì¥ì‹í•  ë•Œ ì‚¬ìš©ë˜ë©°, ë‹¤ì–‘í•œ ìƒ‰ìƒê³¼ ì¬ì§ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n4. íŠ¸ë¦¼: í…Œë‘ë¦¬ë¥¼ ê¾¸ë¯¸ê±°ë‚˜ ì‘í’ˆì˜ ì™¸ê´€ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n5. ìœ ë¦¬ê·¸ë¦‡: ì‘í’ˆì— ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ê½ƒ, ëª¨ë˜, ì»¬ëŸ¬ë“œ ìƒŒë“œ ë“±ì˜ ì¬ë£Œë¥¼ ë‹´ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n6. í´ë¦¬ì‹± ì œí’ˆ: ìœ ë¦¬ì˜ í‘œë©´ì„ ê´‘íƒ ë‚´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n7. ì—ë‚˜ë©œ ê·¸ë¦‡: ìœ ë¦¬ì— ê·¸ë¦¼ì„ ê·¸ë¦¬ê³  ìƒ‰ì¹ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n8. ìš©ì œ: ìœ ë¦¬ ì¡°ê°ì„ ë¶€ì°©í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì¬ë£Œë¥¼ ë¶™ì´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\në‹¤ì–‘í•œ ë„êµ¬ì™€ ì¬ë£Œë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ë¦¬ê³µì˜ˆ ì‘ì—…ì„ ì¦ê¸°ê³  ë‹¤ì–‘í•œ ì‘í’ˆì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in model.transformer.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# for param in model.lm_head.parameters():\n",
        "#     param.requires_grad = True"
      ],
      "metadata": {
        "id": "8d53ahWVG08F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs, tokenizer, max_length=256):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_pairs[idx]\n",
        "        input_text = f\"</s> ì§ˆë¬¸: {question} ë‹µë³€: {answer} </s>\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # labelsë¥¼ input_idsì˜ ë³µì‚¬ë³¸ìœ¼ë¡œ ìƒì„±\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # íŒ¨ë”© í† í°ì— ëŒ€í•œ ì†ì‹¤ì„ ë¬´ì‹œí•˜ê¸° ìœ„í•´ íŒ¨ë”© í† í° ì¸ë±ìŠ¤ë¥¼ -100ìœ¼ë¡œ ì„¤ì •\n",
        "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "WsdH5ZCV0Osd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Datasetê³¼ DataLoader ìƒì„±\n",
        "dataset = QADataset(pairs, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# DataLoaderì—ì„œ ë°°ì¹˜ ìƒ˜í”Œ í™•ì¸\n",
        "for batch in dataloader:\n",
        "    print(batch['input_ids'].shape)  # (batch_size, max_length)\n",
        "    print(batch['attention_mask'].shape)  # (batch_size, max_length)\n",
        "    print(batch['labels'].shape)  # (batch_size, max_length)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFEOzt5h33G8",
        "outputId": "c5358b62-f4a0-46ba-ca3b-c17e46f5fd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 256])\n",
            "torch.Size([16, 256])\n",
            "torch.Size([16, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Fine-tuning"
      ],
      "metadata": {
        "id": "j5RsVxtY4aXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "# Datasetê³¼ DataLoader ì •ì˜\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "total_steps = len(dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snMOEgHA33D8",
        "outputId": "9497ab20-3d4a-4eac-d326-d516a8ad33cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.743095874786377\n",
            "Epoch: 1, Loss: 2.508897542953491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "# JSON íŒŒì¼ ë¡œë“œ í•¨ìˆ˜\n",
        "def load_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "    for entry in data[\"data_info\"]:\n",
        "        question = entry[\"question\"]\n",
        "        answer = entry[\"answer\"][\"contents\"]\n",
        "        pairs.append((question, answer))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Dataset í´ë˜ìŠ¤\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs, tokenizer, max_length=256):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_pairs[idx]\n",
        "        input_text = f\"</s> ì§ˆë¬¸: {question} ë‹µë³€: {answer} </s>\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # labelsë¥¼ input_idsì˜ ë³µì‚¬ë³¸ìœ¼ë¡œ ìƒì„±\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # íŒ¨ë”© í† í°ì— ëŒ€í•œ ì†ì‹¤ì„ ë¬´ì‹œí•˜ê¸° ìœ„í•´ íŒ¨ë”© í† í° ì¸ë±ìŠ¤ë¥¼ -100ìœ¼ë¡œ ì„¤ì •\n",
        "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "train_filepath = '/content/drive/MyDrive/ë©‹ì‚¬ì AI 2ì°¨ ì‹¤ì „ í”„ë¡œì íŠ¸/í•œêµ­ì–´ ì„±ëŠ¥ì´ ê°œì„ ëœ ì´ˆê±°ëŒ€AI ì–¸ì–´ëª¨ë¸ ê°œë°œ ë° ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°/TL_02.RLHFë°ì´í„°/SFTlabel.json'\n",
        "valid_filepath = '/content/drive/MyDrive/ë©‹ì‚¬ì AI 2ì°¨ ì‹¤ì „ í”„ë¡œì íŠ¸/í•œêµ­ì–´ ì„±ëŠ¥ì´ ê°œì„ ëœ ì´ˆê±°ëŒ€AI ì–¸ì–´ëª¨ë¸ ê°œë°œ ë° ë°ì´í„°/Validation/02.ë¼ë²¨ë§ë°ì´í„°/VL/SFTlabel.json'\n",
        "\n",
        "# JSON íŒŒì¼ ë°ì´í„° ë¡œë“œ\n",
        "train_pairs = load_data(train_filepath)\n",
        "valid_pairs = load_data(valid_filepath)\n",
        "\n",
        "# Tokenizer ì´ˆê¸°í™” (ì˜ˆ: Hugging Face KoGPT2 ì‚¬ìš©)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
        "\n",
        "# Dataset ë° DataLoader ìƒì„±\n",
        "train_dataset = QADataset(train_pairs, tokenizer)\n",
        "valid_dataset = QADataset(valid_pairs, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# í•™ìŠµìš© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: Hugging Face GPT)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\").to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 2\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# í•™ìŠµ ë° ê²€ì¦ ë£¨í”„\n",
        "for epoch in range(num_epochs):\n",
        "    # === í•™ìŠµ ë‹¨ê³„ ===\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # === ê²€ì¦ ë‹¨ê³„ ===\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} | Validation Loss: {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "QZk1RxCePs5n",
        "outputId": "9aa50b6b-568c-4415-f508-3f7f0ee00574"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-099624a4201d>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-099624a4201d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"</s> ì§ˆë¬¸: {question} ë‹µë³€: {answer} </s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         encoding = self.tokenizer(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m             )\n\u001b[1;32m   3130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3131\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   3132\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3198\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3199\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3200\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2921\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2923\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2924\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA bot Test"
      ],
      "metadata": {
        "id": "gBvH-BF24c5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'ê·¼ìœ¡ì„ í‚¤ìš°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?'\n",
        "input_text = f\"</s> ì§ˆë¬¸: {text} ë‹µë³€:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVOIKxnLlFPw",
        "outputId": "858c2ce1-bbd8-48f5-b900-c81e3b3331ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ì§ˆë¬¸: ê·¼ìœ¡ì„ í‚¤ìš°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”? ë‹µë³€: ê·¼ìœ¡ì´ í˜•ì„±ë˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ :\n",
            "1. ê· í˜• ì¡íŒ ìš´ë™ ë° ì‹ë‹¨ ê´€ë¦¬ - ê·¼ë ¥ì„ í‚¤ìš°ê¸° ìœ„í•´ ì ì ˆí•œ ìš´ë™ì„ ì‹œì‘í•©ë‹ˆë‹¤. \n",
            "2. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì‹ì‚¬ ì¡°ì ˆí•˜ê¸°, ìŠ¤íŠ¸ë ˆì¹­ê³¼ ê°™ì€ ìœ ì‚°ì†Œ ìš´ë™ê³¼ í•¨ê»˜ í•˜ëŠ” í™œë™ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
            "3. ì‹ ì²´í™œë™ ê³„íš ìˆ˜ë¦½- ê±´ê°•í•œ ì‹ ì²´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ íœ´ì‹ê³¼ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤,\n",
            " ê·œì¹™ì ì¸ ìƒí™œê³¼ ê¾¸ì¤€í•œ ìš´ë™ì€ ê·¼ìœ¡ì˜ í™œë™ì„ ì´‰ì§„í•˜ê³  ê¸´ì¥ì„ ì™„í™”ì‹œí‚¤ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ë˜í•œ, ê³¼ë„í•œ ìŒì‹ì€ í”¼í•´ì•¼ í•©ë‹ˆë‹¤. ìº¥ê±°ë£¨ë‚˜ ë°”ë ›ë” ë“±ì˜ ê°€ë²¼ìš´ ìŠ¤í¬ì¸  í™œë™ì€ ëª¸ì˜ ê· í˜•ì„ ê¹¨ëœ¨ë ¤ ë¶€ìƒì„ ì˜ˆë°©í•˜ê±°ë‚˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'ì²­ì†Œë¥¼ í•  ë•Œ ë¬´ì—‡ì„ ë¨¼ì € í•´ì•¼í• ê¹Œìš”?'\n",
        "input_text = f\"</s> ì§ˆë¬¸: {text} ë‹µë³€:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "ZyQXhYSi33BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cd3bbe-64cc-4259-b597-a2872da7e99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ì§ˆë¬¸: ì²­ì†Œë¥¼ í•  ë•Œ ë¬´ì—‡ì„ ë¨¼ì € í•´ì•¼í• ê¹Œìš”? ë‹µë³€: ì²­ì†Œ í›„ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆë‹¤ :\n",
            "1. ë¨¼ì§€ ì œê±° ë° í´ë¦¬ë‹ì„ ìœ„í•œ ì„¸íŠ¸ ì‚¬ìš©ë²• ì„¤ëª…í•˜ì‹œì˜¤. \n",
            "2. ì„¸íƒê¸° í•„í„°ë§ê³¼ ê³µê¸°ì²­ì •ê¸° ì‚¬ìš©ì„ ìœ„í•´ ì„¸ì²™ê¸°ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì„¸ì œ ë˜ëŠ” ì„¬ìœ ìœ ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹¨ë—í•˜ê²Œ ë‹¦ì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ëª‡ ê°€ì§€ ì˜ˆì‹œì…ë‹ˆë‹¤\n",
            "- íŒ(Tip) - ê¹¨ë—í•œ ë¬¼ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ì˜¤ì—¼ë¬¼ì§ˆì´ë‚˜ ì„¸ê· ì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ë˜í•œ, ì˜¤ì—¼ëœ ë¬¼ë„ ì‚¬ìš©í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\n",
            "3. ì ì ˆí•œ ë¬¼ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë” ì˜ ë‹¦ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤(Therm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ì‚°ë¦¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì´ë©°, í™˜ê²½ ë³´í˜¸ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?\"\n",
        "input_text = f\"</s> ì§ˆë¬¸: {text} ë‹µë³€:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "62XCExZ8cVga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8162494a-50cf-459a-b873-73fe303ecf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ì§ˆë¬¸: ì‚°ë¦¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì´ë©°, í™˜ê²½ ë³´í˜¸ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”? ë‹µë³€: ì‚°ë¦¼ ë³´í˜¸ëŠ” ë‹¤ì–‘í•œ ì—­í• ê³¼ ì±…ì„ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. \n",
            "\n",
            "1. ìƒíƒœê³„ ë³´ì „ ë° ì§€ì† ê°€ëŠ¥í•œ ì´ìš© ì´‰ì§„ : ì‚°ë¦¼ì€ ìƒë¬¼ ë‹¤ì–‘ì„±ê³¼ ìƒíƒœì  ë‹¤ì–‘ì„±ì„ ìœ ì§€í•˜ê³  ìœ ì§€í•˜ëŠ”ë° ì¤‘ìš”í•œ ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
            "ì²«ì§¸, ìƒíƒœê³„ì˜ ë³´ì „ê³¼ ë³µì›\"ì€ ê¸°í›„ë³€í™”ì™€ ìì—°ì¬í•´ë¡œë¶€í„° ë³´í˜¸í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì •ë¶€ëŠ” ìˆ² ê°€ê¾¸ê¸°, ë‚˜ë¬´ ì‹¬ê¸° ë“± ìˆ²ì„ ìœ„í•œ ì‚¬ì—…ì„ ì¶”ì§„í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‚¬ì—…ì€ ìˆ²ì˜ ê°€ì¹˜ë¥¼ ë†’ì´ê³  í™˜ê²½ì„ ë³´í˜¸í•˜ê¸° ë•Œë¬¸ì— ë§ì€ ì‚¬ëŒë“¤ì´ í˜œíƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "ë‘˜ì§¸ë¡œ, ì„ì—… ìƒì‚°ì„± í–¥ìƒ (ì˜ˆ: ëª©ì¬ ìƒì‚°) ì€ ì„ì‚°ë¬¼ ìƒì‚°ì„ í†µí•´ ìˆ˜ìµì„ ì°½ì¶œí•  ìˆ˜ë„ ìˆì§€ë§Œ, ë‹¤ë¥¸\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Save"
      ],
      "metadata": {
        "id": "YhYdBqGS5Jf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = './fine_tuned_kogpt2'\n",
        "\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "LRiF0R2ng9fG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182902f6-ffeb-40ba-b5e7-e102671d8796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_kogpt2/tokenizer_config.json',\n",
              " './fine_tuned_kogpt2/special_tokens_map.json',\n",
              " './fine_tuned_kogpt2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Load"
      ],
      "metadata": {
        "id": "D5Jq8XVp5NVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(save_directory)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNLT5APq5Qa6",
        "outputId": "b16ac2d8-eab0-49a2-d615-6bc7a5443c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'ê·¼ìœ¡ì„ í‚¤ìš°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?'\n",
        "input_text = f\"</s> ì§ˆë¬¸: {text} ë‹µë³€:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjnHmxS5UnW",
        "outputId": "07ceb51f-6e7f-427d-d372-18d9a39212fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ì§ˆë¬¸: ê·¼ìœ¡ì„ í‚¤ìš°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”? ë‹µë³€: ê·¼ìœ¡ì´ ì„±ì¥í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆë‹¤ :\n",
            "1. ìš´ë™ëŸ‰ ì¡°ì ˆ - ê·¼ë ¥ ìš´ë™ì„ í†µí•´ ê·¼ìœ¡ì˜ í˜ì„ í‚¤ì›ë‹ˆë‹¤. \n",
            "2. ìŠ¤íŠ¸ë ˆì¹­ê³¼ ìœ ì‚°ì†Œìš´ë™- ê·¸ë¦¬ê³  ê· í˜• ì¡íŒ ì‹ë‹¨ ìœ ì§€ì™€ ì ì ˆí•œ ìš´ë™ìœ¼ë¡œ ì²´ì¤‘ì„ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
            "3. ìœ ì—°ì„± í–¥ìƒ ë° ë°¸ëŸ°ìŠ¤ ê´€ë¦¬â€“ ìœ ì—°í•œ ì‹ ì²´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë°œë°”ë‹¥, íŒ”ëš ë“± ë‹¤ì–‘í•œ ë¶€ìœ„ì˜ ì›€ì§ì„ì„ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë™ì‘ì„ ë°˜ë³µí•˜ë©´ ë©ë‹ˆë‹¤.\n",
            "4. í˜¸í¡ í›ˆë ¨ì˜ ì¤‘ìš”ì„± í™•ì¸ Â­ í˜¸í¡ì„ í†µí•œ ëª¸ì˜ ì›€ì§ì„ì— ëŒ€í•œ ì´í•´ì™€ í›ˆë ¨ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì¶©ë¶„í•œ íœ´ì‹ê³¼ í•¨ê»˜ ê·œì¹™ì ì¸ ìˆ˜ë©´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ì‚°ë¦¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì´ë©°, í™˜ê²½ ë³´í˜¸ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?\"\n",
        "input_text = f\"</s> ì§ˆë¬¸: {text} ë‹µë³€:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSNbIF_C5Xpv",
        "outputId": "e421d8a3-8a39-47df-a914-e87bcaf02354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s> ì§ˆë¬¸: ì‚°ë¦¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì´ë©°, í™˜ê²½ ë³´í˜¸ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”? ë‹µë³€: ì‚°ë¦¼ì²­ì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ëŒ€í‘œì ì¸ ì„ì—… ê¸°ê´€ì…ë‹ˆë‹¤. \n",
            "\n",
            "1. ìƒíƒœê³„ ë³´ì „ ë° ê´€ë¦¬ : ì‚°ë¦¼ì€ ìƒë¬¼ ë‹¤ì–‘ì„±ê³¼ ìƒíƒœê³„ë¥¼ ë³´í˜¸í•˜ê³ , ìƒíƒœê³„ì˜ ê· í˜•ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìˆ² ê°€ê¾¸ê¸°, ë‚˜ë¬´ ì‹¬ê¸° ë“± ìì—°ì¹œí™”ì ì¸ í™œë™ì„ ì¥ë ¤í•˜ê³  ì§€ì† ê°€ëŠ¥í•œ ê´€ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
            "2. ê¸°í›„ë³€í™” ëŒ€ì‘ê³¼ ìì› ì¬í™œìš© ì´‰ì§„ ë…¸ë ¥ - ì •ë¶€ëŠ” ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œì„ ì¤„ì´ê¸° ìœ„í•œ ë…¸ë ¥ì„ ê¸°ìš¸ì´ê³  ìˆìŠµë‹ˆë‹¤.\n",
            "3. ìˆ˜ì§ˆ ê°œì„  í™œë™ ì§€ì› (ì˜ˆ: ëŒ€ê¸°ì˜¤ì—¼ ì •í™”) ì‚¬ì—…ì€ í•˜ì²œì˜ ìˆ˜ì§ˆì„ ê°œì„ í•˜ê¸° ìœ„í•˜ì—¬ ë¬¼ì„ ê³µê¸‰í•˜ê³  ì˜¤ì—¼ë¬¼ì§ˆì„ ì œê±°í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
            "4. ì¬í•´ ì˜ˆë°©í™œë™ ê°•í™”ì™€ ë³µêµ¬ ê³„íš ìˆ˜ë¦½ì— ëŒ€í•œ ì§€ì›ì„ í†µí•´ êµ­ë¯¼ì˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### enhancing variability"
      ],
      "metadata": {
        "id": "bDQmVX8a8FB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ì‚°ë¦¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì´ë©°, í™˜ê²½ ë³´í˜¸ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?\"\n",
        "input_text = f\"</s> ì§ˆë¬¸: {text} ë‹µë³€:\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "gen_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=128,\n",
        "    repetition_penalty=2.0,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    use_cache=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXp1kGvI7dzP",
        "outputId": "f47f2973-54cd-4fa5-e160-b6640109357b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì§ˆë¬¸: ì‚°ë¦¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì´ë©°, í™˜ê²½ ë³´í˜¸ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”? ë‹µë³€: ì‚°ë¦¼ ìƒíƒœê³„ì˜ ì¤‘ìš”í•œ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ :\n",
            "1. ìì› ë³´ì¡´ ë° ë³´ì „ì— ê´€í•œ ê¸°ë³¸ ì›ì¹™, ë³´ì „ ê³„íš ìˆ˜ë¦½ê³¼ ìœ ì§€ ë³´ìˆ˜ ì ˆì°¨ì˜ ì¤€ìˆ˜ ì—¬ë¶€ ë“±ì…ë‹ˆë‹¤.\n",
            "2. ìƒíƒœê³„ ë³µì›ê³¼ ë³´í˜¸ ê´€ë¦¬ ê³„íšì˜ ìˆ˜ë¦½, ì§€ì† ê°€ëŠ¥í•œ ê´€ë¦¬ë¥¼ ìœ„í•œ ë…¸ë ¥ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
            "3. ê¸°í›„ ë³€í™” ëŒ€ì‘ì— ëŒ€í•œ ê´€ì‹¬ê³¼ ì°¸ì—¬ë„ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ì •ì±… ì œì•ˆì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
            "4. ìƒë¬¼ ë‹¤ì–‘ì„± ë³´í˜¸ë¥¼ í†µí•œ ì„œì‹ì§€ ë³´í˜¸, í† ì–‘ ì˜¤ì—¼ ì˜ˆë°© ë“±ì— ê´€ì‹¬ì„ ê¸°ìš¸ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
            "5. ìì—° ì¬í•´ ì˜ˆë°©ì„ ìœ„í•´ì„œëŠ” ì§€ì†ì ì¸ ê´€ë¦¬ì™€ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
            "6. ëŒ€ê¸° ì§ˆ ê°œì„ ì„ í†µí•´ ìˆ² í›¼ì†ì„ ë°©ì§€í•˜ê³  ìˆ²ì˜ ìƒíƒœì  ê°€ì¹˜ë¥¼ ì¦ì§„\n"
          ]
        }
      ]
    }
  ]
}