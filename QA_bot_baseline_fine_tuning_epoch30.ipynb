{"cells":[{"cell_type":"markdown","metadata":{"id":"Z9Ciu62NpJax"},"source":["# QA bot baseline\n","## Pretrained Model Load(SKT-KoGPT2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4230,"status":"ok","timestamp":1731911886218,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"},"user_tz":-540},"id":"f1qwKQ9vrwLf","outputId":"806db90b-b2cc-4a8e-c507-e90239d2f2ee"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]}],"source":["from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token='</s>',\n","                                                    eos_token='</s>',\n","                                                    unk_token='<unk>',\n","                                                    pad_token='<pad>',\n","                                                    mask_token='<mask>')\n","#tokenizer.tokenize(\"</s> ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o </s>\")"]},{"cell_type":"code","source":["tokenizer.tokenize(\"</s> ë´„ ì— ì…ì„ ì˜· ì¢…ë¥˜ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. </s>\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6J0POqwFKjuA","executionInfo":{"status":"ok","timestamp":1731911886218,"user_tz":-540,"elapsed":8,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"}},"outputId":"51210bbd-1f03-4d63-f741-4a6a8a93dc0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['</s>', 'â–ë´„', 'â–ì—', 'â–ì…ì„', 'â–ì˜·', 'â–ì¢…ë¥˜ë¥¼', 'â–ì¶”ì²œ', 'í•´ì£¼', 'ì„¸', 'ìš”.', 'â–', '</s>']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["tokenizer.tokenize(\"</s> ë´„ì— ì…ì„ ì˜· ì¢…ë¥˜ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. </s>\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2NUY1gJKkeb","executionInfo":{"status":"ok","timestamp":1731911886218,"user_tz":-540,"elapsed":6,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"}},"outputId":"c7911943-08a7-4452-f2f9-b67278aaad71"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['</s>', 'â–ë´„ì—', 'â–ì…ì„', 'â–ì˜·', 'â–ì¢…ë¥˜ë¥¼', 'â–ì¶”ì²œ', 'í•´ì£¼', 'ì„¸', 'ìš”.', 'â–', '</s>']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5195,"status":"ok","timestamp":1731911891409,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"},"user_tz":-540},"id":"DMohjsyl0O3-","outputId":"b1ce4291-5d74-4531-9ca3-025f557c6e02"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(51200, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",")"]},"metadata":{},"execution_count":4}],"source":["import torch\n","from transformers import GPT2LMHeadModel\n","\n","model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"auoElfwb4RfH"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2347,"status":"ok","timestamp":1731911893753,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"},"user_tz":-540},"id":"Ai4WW5rjdkUO","outputId":"3ca02708-a4aa-4995-fd7f-af1524a66e48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojpLLfh50Ozc"},"outputs":[],"source":["import json\n","\n","def load_data(filepath):\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    pairs = []\n","    for entry in data[\"data_info\"]:\n","        question = entry[\"question\"]\n","        answer = entry[\"answer\"][\"contents\"]\n","        pairs.append((question, answer))\n","\n","    return pairs"]},{"cell_type":"code","source":["filepath_test = '/content/drive/MyDrive/Project2/Data/SFTlabel.json'\n","pairs_test = load_data(filepath_test)\n","print(pairs_test[3])"],"metadata":{"id":"T8PUc3kMuVwt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731911893753,"user_tz":-540,"elapsed":13,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"}},"outputId":"ff0734df-531d-4bf5-85d1-aec9d7becff0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('ë„ìê¸°ë¥¼ ê³ ë¥´ê²Œ ëŒë¦¬ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ì˜ ì´ë¦„ì€?', \"ë„ìê¸°ë¥¼ ê³ ë¥´ê²Œ ëŒë¦¬ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ëŠ” 'ë„ë§ˆ'ì…ë‹ˆë‹¤. ë„ë§ˆëŠ” ë„ìê¸°ë¥¼ ê³ ë¥´ê²Œ íšŒì „ì‹œì¼œì£¼ëŠ” ì›í˜•ì˜ ë„êµ¬ë¡œì„œ ì£¼ë¡œ ë„ìê¸° ê³µë°©ì´ë‚˜ ë„ìê¸° ì‘ì—… ê³µê°„ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\")\n"]}]},{"cell_type":"code","source":["filepath_validation = '/content/drive/MyDrive/Project2/Data/SFTlabel_Validation.json'\n","pairs_validation = load_data(filepath_validation)\n","print(pairs_validation[3])"],"metadata":{"id":"OkloMSsCua7g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731911893753,"user_tz":-540,"elapsed":7,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"}},"outputId":"41a57712-af93-4a04-8ee7-a4fb40ab0150"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('ìœ ë¦¬ê³µì˜ˆ ì‘ì—…í•  ë•Œ í•„ìš”í•œ ë„êµ¬ì™€ ì¬ë£Œê°€ ë­ì•¼?', 'ìœ ë¦¬ê³µì˜ˆ ì‘ì—…ì„ ìœ„í•´ í•„ìš”í•œ ë„êµ¬ ë° ì¬ë£ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\në„êµ¬\\n1. ìœ ë¦¬ ì ˆë‹¨ ì¹¼: ìœ ë¦¬ë¥¼ ì›í•˜ëŠ” í¬ê¸°ë¡œ ì ˆë‹¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n2. ì„ ê³¼: ì§ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n3. ì ˆë‹¨ ì ‘ì°©ì œ: ì ‘í•©ì„ ìœ„í•´ ì‚¬ìš©ë˜ë©°, ê°•ë ¥í•œ ì ‘ì°©ì œë¡œ ìœ ë¦¬ ì¡°ê°ì´ ë¶™ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\\n4. ëª¨ë˜ ì§€ì†ê¸°: ìœ ë¦¬ì˜ ëì„ ë‹¤ë“¬ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n5. ì†Œí™”ê¸°: ì‘ì—… ë„ì¤‘ ì‘ì€ ë¶ˆê½ƒì´ ë°œìƒí•  ê²½ìš° ì•ˆì „ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n6. ì—°ë§ˆ ë„êµ¬: ìœ ë¦¬ì˜ í‘œë©´ì„ ì—°ë§ˆí•˜ì—¬ ë‹¤ë“¬ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n7. ë²¤ì¹˜ ê·¸ë¼ì¸ë”: ìœ ë¦¬ë¥¼ ëª¨ì–‘ì„ ë‹¤ë“¬ëŠ” ë° ì‚¬ìš©ë˜ë©°, ëª¨ì–‘ì„ ì¢€ ë” ì •í™•í•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n8. í˜ì¸íŠ¸ ë¸ŒëŸ¬ì‹œ: ë””ìì¸ê³¼ ìƒ‰ìƒì„ ìœ ë¦¬ì— ê·¸ë¦´ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\nì¬ë£Œ\\n1. ìœ ë¦¬ ì¡°ê°: ë‹¤ì–‘í•œ í¬ê¸°, ëª¨ì–‘, ìƒ‰ìƒì˜ ìœ ë¦¬ ì¡°ê°ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n2. í…Œë‘ë¦¬: ìœ ë¦¬ ì‘í’ˆì„ ë³´í˜¸í•˜ê±°ë‚˜ ê²¬ê³ í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n3. ì¤„ë¬´ëŠ¬: ì‘í’ˆì„ ê±¸ê±°ë‚˜ ì¥ì‹í•  ë•Œ ì‚¬ìš©ë˜ë©°, ë‹¤ì–‘í•œ ìƒ‰ìƒê³¼ ì¬ì§ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n4. íŠ¸ë¦¼: í…Œë‘ë¦¬ë¥¼ ê¾¸ë¯¸ê±°ë‚˜ ì‘í’ˆì˜ ì™¸ê´€ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n5. ìœ ë¦¬ê·¸ë¦‡: ì‘í’ˆì— ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ê½ƒ, ëª¨ë˜, ì»¬ëŸ¬ë“œ ìƒŒë“œ ë“±ì˜ ì¬ë£Œë¥¼ ë‹´ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n6. í´ë¦¬ì‹± ì œí’ˆ: ìœ ë¦¬ì˜ í‘œë©´ì„ ê´‘íƒ ë‚´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n7. ì—ë‚˜ë©œ ê·¸ë¦‡: ìœ ë¦¬ì— ê·¸ë¦¼ì„ ê·¸ë¦¬ê³  ìƒ‰ì¹ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n8. ìš©ì œ: ìœ ë¦¬ ì¡°ê°ì„ ë¶€ì°©í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì¬ë£Œë¥¼ ë¶™ì´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\në‹¤ì–‘í•œ ë„êµ¬ì™€ ì¬ë£Œë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ë¦¬ê³µì˜ˆ ì‘ì—…ì„ ì¦ê¸°ê³  ë‹¤ì–‘í•œ ì‘í’ˆì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!')\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d53ahWVG08F"},"outputs":[],"source":["# for param in model.transformer.parameters():\n","#     param.requires_grad = False\n","\n","# for param in model.lm_head.parameters():\n","#     param.requires_grad = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsdH5ZCV0Osd"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class QADataset(Dataset):\n","    def __init__(self, qa_pairs, tokenizer, max_length=256):\n","        self.qa_pairs = qa_pairs\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.qa_pairs)\n","\n","    def __getitem__(self, idx):\n","        question, answer = self.qa_pairs[idx]\n","        input_text = f\"</s> ì§ˆë¬¸: {question} ë‹µë³€: {answer} </s>\"\n","\n","        encoding = self.tokenizer(\n","            input_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        input_ids = encoding['input_ids'].squeeze()\n","        attention_mask = encoding['attention_mask'].squeeze()\n","\n","        # labelsë¥¼ input_idsì˜ ë³µì‚¬ë³¸ìœ¼ë¡œ ìƒì„±\n","        labels = input_ids.clone()\n","\n","        # íŒ¨ë”© í† í°ì— ëŒ€í•œ ì†ì‹¤ì„ ë¬´ì‹œí•˜ê¸° ìœ„í•´ íŒ¨ë”© í† í° ì¸ë±ìŠ¤ë¥¼ -100ìœ¼ë¡œ ì„¤ì •\n","        labels[input_ids == self.tokenizer.pad_token_id] = -100\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels\n","        }\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfnEYSCqp2Fe"},"outputs":[],"source":["from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4mKerzfIp0ie"},"outputs":[],"source":["train_dataset = QADataset(pairs_test, tokenizer)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","val_dataset = QADataset(pairs_validation, tokenizer)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1731911894162,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"},"user_tz":-540},"id":"WFEOzt5h33G8","outputId":"50c549dc-abf9-41ef-e224-c441ca7d56c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["662\n","10580\n","torch.Size([16, 256])\n","torch.Size([16, 256])\n","torch.Size([16, 256])\n","83\n","1322\n","torch.Size([16, 256])\n","torch.Size([16, 256])\n","torch.Size([16, 256])\n"]}],"source":["# DataLoaderì—ì„œ ë°°ì¹˜ ìƒ˜í”Œ í™•ì¸\n","\n","print(len(train_dataloader))\n","print(len(train_dataloader.dataset))\n","\n","for batch in train_dataloader:\n","    print(batch['input_ids'].shape)  # (batch_size, max_length)\n","    print(batch['attention_mask'].shape)  # (batch_size, max_length)\n","    print(batch['labels'].shape)  # (batch_size, max_length)\n","    break\n","\n","print(len(val_dataloader))\n","print(len(val_dataloader.dataset))\n","\n","for batch in val_dataloader:\n","    print(batch['input_ids'].shape)  # (batch_size, max_length)\n","    print(batch['attention_mask'].shape)  # (batch_size, max_length)\n","    print(batch['labels'].shape)  # (batch_size, max_length)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"j5RsVxtY4aXu"},"source":["## Model Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vduhiwmp7fiE"},"outputs":[],"source":["def save_model(model, tokenizer, epoch):\n","  save_directory = '/content/drive/MyDrive/Project2/fine_tuned2_kogpt2_epoch' + str(epoch+1)\n","\n","  model.save_pretrained(save_directory)\n","  tokenizer.save_pretrained(save_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":758},"id":"snMOEgHA33D8","outputId":"153ddf54-7e79-4316-b3fe-b3f9f816242a","executionInfo":{"status":"error","timestamp":1731915376706,"user_tz":-540,"elapsed":3479941,"user":{"displayName":"ì„ìŠ¹ìˆ˜","userId":"07868405882142888382"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 2.5498781204223633, Validation Loss: 2.6303213619324097\n","Epoch: 1, Loss: 2.5196614265441895, Validation Loss: 2.554392087890441\n","Epoch: 2, Loss: 2.2004594802856445, Validation Loss: 2.5338284710803665\n","Epoch: 3, Loss: 2.026648759841919, Validation Loss: 2.5389505380607513\n","Epoch: 4, Loss: 1.7974423170089722, Validation Loss: 2.549360525177186\n","Epoch: 5, Loss: 2.128162384033203, Validation Loss: 2.56954034839768\n","Epoch: 6, Loss: 1.9983564615249634, Validation Loss: 2.598156104604882\n","Epoch: 7, Loss: 1.7069597244262695, Validation Loss: 2.6202958664262153\n","Epoch: 8, Loss: 1.5984896421432495, Validation Loss: 2.6580891896443193\n","Epoch: 9, Loss: 1.41572105884552, Validation Loss: 2.6896425902125345\n","Epoch: 10, Loss: 1.3024977445602417, Validation Loss: 2.726694095565612\n","Epoch: 11, Loss: 1.3147656917572021, Validation Loss: 2.758708973965013\n","Epoch: 12, Loss: 1.1436717510223389, Validation Loss: 2.7997053387653397\n","Epoch: 13, Loss: 0.9771098494529724, Validation Loss: 2.822834138410637\n","Epoch: 14, Loss: 1.0590531826019287, Validation Loss: 2.8623261394270934\n","Epoch: 15, Loss: 1.0168726444244385, Validation Loss: 2.8910653275179574\n","Epoch: 16, Loss: 0.8337737321853638, Validation Loss: 2.924210706389094\n","Epoch: 17, Loss: 0.7476286292076111, Validation Loss: 2.9521899338228157\n","Epoch: 18, Loss: 0.8373007774353027, Validation Loss: 2.978858284203403\n","Epoch: 19, Loss: 0.7808645367622375, Validation Loss: 3.0028041299567163\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-c086108a48ad>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","num_epochs = 30\n","\n","\n","# Datasetê³¼ DataLoader ì •ì˜\n","# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","\n","# ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","total_steps = len(train_dataloader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)\n","\n","# í•™ìŠµ ë£¨í”„\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids=input_ids, labels=labels)\n","        loss = outputs.loss\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()  # ê²€ì¦ ëª¨ë“œ\n","    val_loss = 0\n","    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n","        for batch in val_dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, labels=labels)\n","            val_loss += outputs.loss.item()\n","\n","    val_loss /= len(val_dataloader)  # í‰ê·  ê²€ì¦ ì†ì‹¤ ê³„ì‚°\n","    print(f\"Epoch: {epoch}, Loss: {loss.item()}, Validation Loss: {val_loss}\")\n","\n","    if (epoch + 1) % 5 == 0:\n","        save_model(model, tokenizer, epoch)"]},{"cell_type":"markdown","metadata":{"id":"YhYdBqGS5Jf6"},"source":["## Finish"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrRZhD8X4h50"},"outputs":[],"source":["# ì½”ë© ì„¸ì…˜ì„ ì¢…ë£Œì‹œì¼œì£¼ì„¸ìš”\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1pnrPWELesFwKZSZH7lb4joxRMdvFSIuz","timestamp":1731477979257}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}